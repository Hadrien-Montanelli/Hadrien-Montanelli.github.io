<!DOCTYPE html>
	<html>
		<head>
			<title>Deep networks and the curse of dimensionality</title>
			<!-- link to main stylesheet -->
			<link rel="stylesheet" type="text/css" href="/css/main.css">
			<!-- Global site tag (gtag.js) - Google Analytics -->
			<script async src="https://www.googletagmanager.com/gtag/js?id=UA-108246943-1"></script>
			<script>
			  window.dataLayer = window.dataLayer || [];
			  function gtag(){dataLayer.push(arguments);}
			  gtag('js', new Date());
			  gtag('config', 'UA-108246943-1');
			</script>
		</head>
		<body>
			<nav>
	    		<ul>
	    			

					<li>
						<a href="/" data-current="blog" >
							Home
						</a>
					</li>
					<li>
						<a href="/about" >
							About
						</a>
					</li>
					<li>
						<a href="/awards" >
							Awards
						</a>
					</li>
					<li>
						<a href="/blog"  class="active" >
							Blog
						</a>
					</li>
					<li>
						<a href="/publications" >		Publications
						</a>
					</li>
					<li>
						<a href="/talks" >
							Talks
						</a>
					</li>
					<li>
						<a href="/teaching" >
							Teaching
						</a>
					</li>
	    		</ul>
			</nav>
			<div class="container">
			
			<div style="text-align: center;">
  <h1>Deep networks and the curse of dimensionality</h1>
  <p class="meta">22 Dec 2017</p>
</div>


<div class="post">
  <div style="text-align: center;">
	<img src="/blog/deepnet.jpg" style="width:595px;height:300px;" />
</div>

<p>I started working on <a href="http://en.wikipedia.org/wiki/Deep_learning">deep learning</a> 
models when I began my research at <a href="http//www.columbia.edu">Columbia University</a> in September 2017. 
Coming from the <a href="http://en.wikipedia.org/wiki/Approximation_theory">approximation theory</a>
world, I wanted to learn about their approximation propeties.
A few months down the road I have proven a <a href="http://arxiv.org/pdf/1712.08688.pdf">new theorem</a> concerning the approximation of 
<a href="http://en.wikipedia.org/wiki/Function_of_several_real_variables">multivariate functions</a> 
by deep ReLU networks, which partly explained why deep networks are so powerfull.</p>

<p>Deep learning has been successfully applied to many fields including 
<a href="https://en.wikipedia.org/wiki/Computer_vision">computer vision</a>, 
<a href="https://en.wikipedia.org/wiki/Speech_recognition">speech recognition</a>, 
and <a href="http://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>.
It is based on approximations by <i><b>deep networks</b></i>, as opposed to <i><b>shallow networks</b></i>.
The latter are neural networks with a single <i><b>layer</b></i> and correspond to approximations of the form</p>

<script type="math/tex; mode=display">f_N(\mathbf{x}) = \sum_{i=1}^N \alpha_i \sigma(\mathbf{w}_i^T\mathbf{x} + \theta_i), 
\quad \alpha_i,\,\theta_i\in\mathbb{R}, \, \mathbf{w}_i\in\mathbb{R}^d,</script>

<p>for some <i><b>activation function</b></i> <script type="math/tex">\sigma:\mathbb{R}\rightarrow\mathbb{R}</script>, 
while the former are neural networks with one or more layers, where each unit of each layer 
performs an operation of the form <script type="math/tex">\sigma(\mathbf{w}\cdot \mathbf{x} + \theta)</script>.
The <i><b>depth</b></i> of a network is its number of layers and its <i><b>size</b></i> 
is its total number of units.
Shallow networks have depth <script type="math/tex">1</script> and their size is the number <script type="math/tex">N</script> in the expansion above,
while deep networks usually have depth <script type="math/tex">\gg 1</script>.
Deep <i><b>ReLU</b></i> networks use the activation function <script type="math/tex">\sigma(x) = \max(0,x)</script>.</p>

<p>One of the most important theoretical problems is to determine why and when deep (but not shallow) networks
can lessen or break the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>.
A possible way of addressing this problem is to focus on a particular set of functions which have a very
special (<a href="http://en.wikipedia.org/wiki/Function_composition">compositional</a> or 
<a href="https://en.wikipedia.org/wiki/Polynomial">polynomial</a>) structure, 
and to show that for this particular set deep networks perform extremely well.
I have followed a different route. 
I have considered a generic set of functions and proved new error estimates 
for which the curse of dimensionality is lessened by establishing a connection with 
<a href="http://en.wikipedia.org/wiki/Sparse_grid">sparse grids</a>.</p>

<p>For a real-valued function <script type="math/tex">f</script> in <script type="math/tex">\mathbb{R}^d</script> whose 
<a href="http://en.wikipedia.org/wiki/Smoothness">smoothness</a> is characterized by some integer <script type="math/tex">m</script>
(e.g., the number of <a href="https://en.wikipedia.org/wiki/Locally_integrable_function">integrable</a> derivatives), and for a neural network <script type="math/tex">f_N</script> of size <script type="math/tex">N</script>, 
one tries to find the asymptotic behavior of the approximation error as <script type="math/tex">N\rightarrow+\infty</script>,</p>

<script type="math/tex; mode=display">\Vert f - f_N \Vert = \mathcal{O}(N^{-\frac{m}{d}}) 
\quad \Longleftrightarrow \quad \Vert f - f_N \Vert \leq \epsilon \quad \text{whenever} \quad N=\mathcal{O}(\epsilon^{-\frac{d}{m}}),</script>

<p>for some norm <script type="math/tex">\Vert\cdot\Vert</script>. 
For deep networks, one also wants to find the asymptotic behavior of the depth 
as a function of the accuracy <script type="math/tex">\epsilon</script>.
Results of this form are standard approximation results that suffer from the curse of dimensionality.
These apply to, e.g., multivariate polynomial approximation.
As the size of the network <script type="math/tex">N</script> increases, the approximation error goes to zero, <i><b>the smoother the function the faster</b></i>, but the convergence becomes <i><b>geometrically slower</b></i> as the dimension <script type="math/tex">d</script> increases.</p>

<p>The picture is different for deep networks.
My theorem shows that functions in the so-called Korobov spaces <script type="math/tex">X^{2,p}(\Omega)</script> of mixed derivatives of order <script type="math/tex">2</script> can be represented to accuracy <script type="math/tex">\epsilon</script> by deep networks of depth 
<script type="math/tex">\mathcal{O}(\vert\log_2\epsilon\vert\log_2d)</script> and size</p>

<script type="math/tex; mode=display">N=\mathcal{O}(\epsilon^{-\frac{1}{2}}\vert\log_2\epsilon\vert^{\frac{3}{2}(d-1)+1}\log_2d).</script>

<p>The curse of dimensionality is not totally overcome but is lessened since 
<script type="math/tex">d</script> only affects logarithmic factors <script type="math/tex">\vert\log_2\epsilon\vert</script>.
This result might explain why deep networks work so well.</p>

</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" 
    type="text/javascript">
</script>

			
			</div><!-- /.container -->
			<footer>
	    		<ul>	
				<li><a href="http://www.linkedin.com/in/hadrien-montanelli-137b7958/">LinkedIn</a></li>
				<li><a href="http://scholar.google.com/citations?user=Bjmkfe8AAAAJ&hl=en&oi=sra/">Scholar</a></li>
			</ul>
			</footer>
		</body>
	</html>
