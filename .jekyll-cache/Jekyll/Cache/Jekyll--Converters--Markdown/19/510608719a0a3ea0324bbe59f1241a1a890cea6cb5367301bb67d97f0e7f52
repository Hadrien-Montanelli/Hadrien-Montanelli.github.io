I"<h3>Kolmogorov&ndash;Arnold superposition theorem</h3>

<p>In my latest <a href="http://arxiv.org/pdf/1906.11945.pdf">paper</a>, I prove a theorem about the <a href="http://en.wikipedia.org/wiki/Approximation_theory">approximation</a> of <a href="https://en.wikipedia.org/wiki/Function_of_several_real_variables">multivariate</a> continuous functions by <a href="http://en.wikipedia.org/wiki/Deep_learning">deep ReLU networks</a>, for which the <a href="http://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a> is lessened. My theorem is based on the <a href="http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem">Kolmogorov–Arnold superposition theorem</a>, and on the approximation of the inner and outer functions that appear in the superposition by very deep ReLU networks.</p>

<h2>Kolmogorov&ndash;Arnold superposition theorem</h2>

<p>At the second <a href="http://en.wikipedia.org/wiki/International_Congress_of_Mathematicians">International Congress of Mathematicians</a> in Paris 1900, <a href="http://en.wikipedia.org/wiki/David_Hilbert">Hilbert</a> presented ten of his <a href="http://en.wikipedia.org/wiki/Hilbert%27s_problems">23 problems</a>, including the <a href="http://en.wikipedia.org/wiki/Hilbert%27s_thirteenth_problem">13th problem</a> about equations of degree seven. He considered the following equation,</p>

<script type="math/tex; mode=display">x^7 + ax^3 + bx^2 + cx + 1 = 0,</script>

<p>and asked whether its solution <script type="math/tex">x(a,b,c)</script>, seen as a function of the three parameters <script type="math/tex">a</script>, <script type="math/tex">b</script> and <script type="math/tex">c</script>, can be written as the <a href="http://en.wikipedia.org/wiki/Function_composition">composition</a> of functions of only two variables.</p>

<p>Hilbert’s 13th problem was solved by <a href="http://en.wikipedia.org/wiki/Andrey_Kolmogorov">Kolmogorov</a> and his 19 years old student <a href="http://en.wikipedia.org/wiki/Vladimir_Arnold">Arnold</a> in a series of papers in the 1950s. Kolmogorov first proved in 1956 that any continuous function of several variables can be expressed as the composition of functions of three variables. His student Arnold extended his theorem in 1957; three variables were reduced to two. Kolmogorov finally showed later that year that only functions of one variable were needed. The latter result is known as the Kolmogorov–Arnold superposition theorem, and states that any continuous functions <script type="math/tex">f:[0,1]^n\rightarrow\mathbb{R}</script> can be decomposed as</p>

<script type="math/tex; mode=display">f(x_1,\ldots,x_n) = \sum_{j=0}^{2n}\phi_j\left(\sum_{i=1}^n\psi_{i,j}(x_i)\right),</script>

<p>with <script type="math/tex">2n+1</script> continuous <i><b>outer</b></i> functions <script type="math/tex">\phi_j:\mathbb{R}\rightarrow\mathbb{R}</script> (dependent of <script type="math/tex">f</script>) and <script type="math/tex">2n^2+n</script> continuous <i><b>inner</b></i> functions <script type="math/tex">\psi_{i,j}:[0,1]\rightarrow\mathbb{R}</script> (independent of <script type="math/tex">f</script>).</p>

<p>The Kolmogorov–Arnold superposition theorem was further improved in the 1960s and the 1970s. Lorentz showed in 1962 that the outer functions <script type="math/tex">\phi_j</script> might be chosen to be the same function <script type="math/tex">\phi</script>, and replaced the inner functions <script type="math/tex">\psi_{i,j}</script> by <script type="math/tex">\lambda_i\psi_j</script>, for some positive rationally independent constants <script type="math/tex">\lambda_i\leq 1</script>, while Sprecher replaced the inner functions <script type="math/tex">\psi_{i,j}</script> by <a href="https://en.wikipedia.org/wiki/H%C3%B6lder_condition">Hölder continuous</a> functions <script type="math/tex">x_i\mapsto\lambda^{ij}\psi(x_i+j\epsilon)</script> in 1965. Two years later, Fridman demonstrated that the inner functions could be chosen to be <a href="https://en.wikipedia.org/wiki/Lipschitz_continuity">Lipschitz continuous</a>, but his decomposition used <script type="math/tex">2n+1</script> outer functions and <script type="math/tex">2n^2+n</script> inner functions. Finally, Sprecher provided in 1972 a decomposition with Lipschitz continuous functions <script type="math/tex">x_i\mapsto\lambda^{i-1}\psi(x_i+j\epsilon)</script>. (See my paper for the references.)</p>

<h2>Approximation theory for deep ReLU networks</h2>

<p>One of the most important theoretical problems in deep network approximation theory is to determine why and when deep networks lessen or break the <a href="http://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>, characterized by the <script type="math/tex">\mathcal{O}(\epsilon^{-n})</script> growth of the network size <script type="math/tex">W</script> as the error <script type="math/tex">\epsilon\rightarrow0</script>, in dimension <script type="math/tex">n</script>.</p>

<p>In my paper, I use a variant of Sprecher’s 1965 version of the theorem, which reads</p>

<script type="math/tex; mode=display">f(x_1,\ldots,x_n) = \sum_{j=0}^{2n}\phi_j\left(\sum_{i=1}^n\lambda_i\psi(x_i+ja)\right),</script>

<p>for some constants <script type="math/tex">\lambda_1=1>\lambda_2>\ldots>\lambda_n</script> and <script type="math/tex">a=[(2n+1)(2n+2)]^{-1}</script>, and with Hölder continuous inner function <script type="math/tex">\psi</script>.</p>

<p>I first show that the outer functions may be approximated by Lipschitz continuous functions. Then, I prove that the inner and outer functions can be approximated with error <script type="math/tex">\epsilon</script> by deep networks of size <script type="math/tex">\mathcal{O}(\epsilon^{-\log n})</script> and <script type="math/tex">\mathcal{O}(\epsilon^{-1/2})</script>, respectively. The resulting network that approximates <script type="math/tex">f</script> has depth and size <script type="math/tex">\mathcal{O}(\epsilon^{-\log n})</script>; the curse of dimensionality is lessened. However, the constants that appear in the esmiates may be considerably large, and the resulting network architecture is adaptive, since the network used in the proof depends on <script type="math/tex">f</script>, via the outer functions. <a href="http://arxiv.org/pdf/1906.11945.pdf">Check it out</a> if you are interested!</p>
:ET