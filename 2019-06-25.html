<!DOCTYPE html>
<html lang="en">
<head>

<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=Edge">
<meta name="description" content="">
<meta name="keywords" content="">
<meta name="author" content="">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<title>Deep networks and the Kolmogorov&ndash;Arnold theorem</title>
<!--

Template 2085 Neuron

http://www.tooplate.com/view/2085-neuron

-->
<link rel="stylesheet" href="css/bootstrap.min.css">
<link rel="stylesheet" href="css/font-awesome.min.css">
<link rel="stylesheet" href="css/magnific-popup.css">

<!-- Main css -->
<link rel="stylesheet" href="css/style.css">
<link href="https://fonts.googleapis.com/css?family=Lora|Merriweather:300,400" rel="stylesheet">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-108246943-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-108246943-1');
</script>

</head>
<body>

<!-- PRE LOADER -->

<div class="preloader">
     <div class="sk-spinner sk-spinner-wordpress">
          <span class="sk-inner-circle"></span>
     </div>
</div>

<!-- Navigation section  -->

<div class="navbar navbar-default navbar-static-top" role="navigation">
     <div class="container">

          <div class="navbar-header">
               <button class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                    <span class="icon icon-bar"></span>
                    <span class="icon icon-bar"></span>
                    <span class="icon icon-bar"></span>
               </button>
               <a href="index.html" class="navbar-brand">H. Montanelli</a>
          </div>
          <div class="collapse navbar-collapse">
               <ul class="nav navbar-nav navbar-right">
                    <li><a href="index.html">Home</a></li>
                    <li><a href="about.html">About</a></li>
                    <li><a href="publications.html">Publications</a></li>
                    <li><a href="talks.html">Talks</a></li>
                    <li><a href="teaching.html">Teaching</a></li>
               </ul>
          </div>

  </div>
</div>

<!-- Home Section -->

<section id="home" class="main-about parallax-section">
     <div class="overlay"></div>
     <div class="container">
          <div class="row">

               <div class="col-md-12 col-sm-12">
                    <h1>Deep networks and the Kolmogorov&ndash;Arnold theorem</h1>
               </div>

          </div>
     </div>
</section>

<!-- Blog Single Post Section -->

<section id="about">
     <div class="container">
          <div class="row">

               <div class="col-md-offset-1 col-md-10 col-sm-12">     
                    <p>In my latest <a href="http://arxiv.org/pdf/1906.11945.pdf">paper</a>, my colleague and I prove a theorem about the <a href='http://en.wikipedia.org/wiki/Approximation_theory'>approximation</a> of <a href='https://en.wikipedia.org/wiki/Function_of_several_real_variables'>multivariate</a> functions by <a href='http://en.wikipedia.org/wiki/Deep_learning'>deep ReLU networks</a>, for which the <a href='http://en.wikipedia.org/wiki/Curse_of_dimensionality'>curse of dimensionality</a> is lessened. Our theorem is based on a constructive proof of the <a href='http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem'>Kolmogorov&ndash;Arnold superposition theorem</a>, and on a subset of multivariate continuous functions whose outer superposition functions can be efficiently approximated by deep ReLU networks.</p>

                    <h2>Kolmogorov&ndash;Arnold superposition theorem</h2>

                    <p>At the second <a href='http://en.wikipedia.org/wiki/International_Congress_of_Mathematicians'>International Congress of Mathematicians</a> in Paris 1900, <a href='http://en.wikipedia.org/wiki/David_Hilbert'>Hilbert</a> presented ten of his <a href='http://en.wikipedia.org/wiki/Hilbert%27s_problems'>23 problems</a>, including the <a href='http://en.wikipedia.org/wiki/Hilbert%27s_thirteenth_problem'>13th problem</a> about equations of degree seven. He considered the following equation,
                    $$
                    x^7 + ax^3 + bx^2 + cx + 1 = 0,
                    $$
                    and asked whether its solution \(x(a,b,c)\), seen as a function of the three parameters \(a\), \(b\) and \(c\), can be written as the <a href='http://en.wikipedia.org/wiki/Function_composition'>composition</a> of functions of only two variables.</p>

                    <p>Hilbert's 13th problem was solved by <a href='http://en.wikipedia.org/wiki/Andrey_Kolmogorov'>Kolmogorov</a> and his 19 years old student <a href='http://en.wikipedia.org/wiki/Vladimir_Arnold'>Arnold</a> in a series of papers in the 1950s. Kolmogorov first proved in 1956 that any continuous function of several variables can be expressed as the composition of functions of three variables. His student Arnold extended his theorem in 1957; three variables were reduced to two. Kolmogorov finally showed later that year that only functions of one variable were needed. The latter result is known as the Kolmogorov&ndash;Arnold superposition theorem, and states that any continuous functions \(f:[0,1]^n\rightarrow\mathbb{R}\) can be decomposed as
                    $$
                    f(x_1,\ldots,x_n) = \sum_{j=0}^{2n}\phi_j\left(\sum_{i=1}^n\psi_{i,j}(x_i)\right),
                    $$
                    with \(2n+1\) continuous <i>outer</i> functions \(\phi_j:\mathbb{R}\rightarrow\mathbb{R}\) (dependent of \(f\)) and \(2n^2+n\) continuous <i>inner</i> functions \(\psi_{i,j}:[0,1]\rightarrow\mathbb{R}\) (independent of \(f\)).</p>

                    <p>The Kolmogorov&ndash;Arnold superposition theorem was further improved in the 1960s and the 1970s. Lorentz showed in 1962 that the outer functions \(\phi_j\) might be chosen to be the same function \(\phi\), and replaced the inner functions \(\psi_{i,j}\) by \(\lambda_i\psi_j\), for some positive rationally independent constants \(\lambda_i\leq 1\), while Sprecher replaced the inner functions \(\psi_{i,j}\) by <a href='https://en.wikipedia.org/wiki/H%C3%B6lder_condition'>H&#246;lder continuous</a> functions \(x_i\mapsto\lambda^{ij}\psi(x_i+j\epsilon)\) in 1965. Two years later, Fridman demonstrated that the inner functions could be chosen to be <a href='https://en.wikipedia.org/wiki/Lipschitz_continuity'>Lipschitz continuous</a>, but his decomposition used \(2n+1\) outer functions and \(2n^2+n\) inner functions. Finally, Sprecher provided in 1972 a decomposition with Lipschitz continuous functions \(x_i\mapsto\lambda^{i-1}\psi(x_i+j\epsilon)\). (See my paper for the references.)</p>

                    <h2>Approximation theory for deep ReLU networks</h2>

                    <p>One of the most important theoretical problems in deep network approximation theory is to determine why and when deep networks lessen or break the <a href='http://en.wikipedia.org/wiki/Curse_of_dimensionality'>curse of dimensionality</a> for multivariate continuous functions, characterized by the \(\mathcal{O}(\epsilon^{-n})\) growth of the network size, as the error \(\epsilon\rightarrow0\).</p>

                    <p>In our paper, we use a variant of Sprecher's 1965 version of the theorem, which reads
                    $$
                    f(x_1,\ldots,x_n) = \sum_{j=0}^{2n}\phi_j\left(\sum_{i=1}^n\lambda_i\psi(x_i+ja)\right),
                    $$
                    for some constants \(\lambda_1=1>\lambda_2>\ldots>\lambda_n\) and \(a=[(2n+1)(2n+2)]^{-1}\), and with H&#246;lder continuous inner function \(\psi\).</p>

                    <p>We first show that the outer functions may be approximated by Lipschitz continuous functions. Then, we prove that&mdash;for a particular subset of the continuous functions&mdash;the inner and outer functions can be approximated with error \(\epsilon\) by deep networks of size \(\mathcal{O}(\epsilon^{-\log n})\) and \(\mathcal{O}(\epsilon^{-1/2})\). The resulting network that approximates \(f\) has depth and size \(\mathcal{O}(\epsilon^{-\log n})\); the curse of dimensionality is lessened. <a href="http://arxiv.org/pdf/1906.11945.pdf">Check it out</a> if you are interested!</p>
               </div>

          </div>
     </div>
</section>

<!-- Footer Section -->

<footer>
     <div class="container">
          <div class="row">

               <div class="col-md-4 col-md-offset-1 col-sm-6">
                    <h3>Contact</h3>
                    <p style="color:white"><i class="fa fa-send-o"></i>hadrien.montanelli@gmail.com</p>
               </div>

               <div class="col-md-5 col-md-offset-1 col-sm-6">
                    <h3 style="opacity:0;">Contact</h3>
                    <p style="color:white">Copyright &copy; 2022 Hadrien Montanelli</p>
               </div>

               <div class="clearfix col-md-12 col-sm-12">
                    <hr style="color:white;">
               </div>

               <div class="col-md-12 col-sm-12">
                    <ul class="social-icon">
                         <li><a href="https://github.com/Hadrien-Montanelli" class="fa fa-github" style="font-size:30px;color:white"></a></li>
                         <li><a href="https://scholar.google.com/citations?user=Bjmkfe8AAAAJ&hl=en&oi=sra/" class="fa fa-google" style="font-size:30px;color:white"></a></li>
                         <li><a href="https://www.linkedin.com/in/hadrien-montanelli/" class="fa fa-linkedin" style="font-size:30px;color:white"></a></li>
                         <li><a href="https://twitter.com/drmontanelli" class="fa fa-twitter" style="font-size:30px;color:white"></a></li>
                    </ul>
               </div>
               
          </div>
     </div>
</footer>

<!-- Back top -->
<a href="#back-top" class="go-top"><i class="fa fa-angle-up"></i></a>

<!-- SCRIPTS -->

<script src="js/jquery.js"></script>
<script src="js/bootstrap.min.js"></script>
<script src="js/particles.min.js"></script>
<script src="js/app.js"></script>
<script src="js/jquery.parallax.js"></script>
<script src="js/smoothscroll.js"></script>
<script src="js/custom.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

</body>
</html>