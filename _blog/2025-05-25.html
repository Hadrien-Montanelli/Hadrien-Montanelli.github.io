---
layout: post
title: When theory meets practice&mdash;and they argue productively
date: 2025-05-25
tags: [research life]
permalink: 2025-05-25.html
---
<p>Welcome to the second entry in my <i>Holiday in Sicily</i> series. Today, I reflect on the interplay between algorithms and theory&mdash;how practice often leads, theory follows, and how the two shape each other in a dynamic loop that drives much of applied math progress.</p>

<h2>Introduction</h2>

<p>In many fields of applied mathematics and engineering, the relationship between theory and practice isn’t a one-way street. It’s a conversation. At its best, it’s a lively argument.</p>

<p>Often, the most exciting developments come from this back-and-forth&mdash;a dynamic feedback loop between algorithms and theory. Practice speaks first: a new method appears, driven by need or intuition. Then theory replies, asking tough questions. Does it work? When? Why?</p>

<p>Sometimes, theory pushes back, exposing flaws or failure modes. Practice listens, adjusts, and counters with a better design. And so the exchange continues&mdash;design, analysis, failure, improvement. A real dialogue. Each side learning, challenging, shaping the other.</p>

<h2>The pattern</h2>

<p>This pattern can be broken down into five stages:</p>

<ul>
  <li><b>It starts with an algorithm.</b><br>Someone proposes a new method. Maybe it’s computationally efficient, maybe it’s inspired by physics, maybe it’s a neural network architecture copied from another domain. The key point: it’s motivated by what works, not necessarily by what can be proved.</li>
  <li><b>Then comes theory.</b><br>After initial empirical validation, theoretical work seeks to rigorously understand the algorithm: Does it converge? Under what conditions? How stable is it? Is it optimal? These proofs don’t just validate the method&mdash;they clarify its domain of applicability and often reveal surprising subtleties.</li>
  <li><b>Theory explains failure modes.</b><br>Theory doesn’t just confirm what works. It explains why it fails under certain conditions. It identifies the edge cases: when the matrix is ill-conditioned, when the PDE becomes singular, when the neural network can’t generalize. These insights often reveal flaws in the original method&mdash;and point the way forward.</li>
  <li><b>Failures lead to new designs.</b><br>Armed with new theoretical insight, practitioners redesign the algorithm. They regularize it. They precondition it. They add skip connections, or stabilize the discretization, or adjust the loss function. Now there’s a new method&mdash;a fix, a patch, or sometimes a breakthrough.</li>
  <li><b>Theory circles back.</b><br>The cycle completes as the community begins to rigorously analyze these new approaches, proving convergence, stability, or optimality. The feedback loop spins again&mdash;and the field takes another step forward. </li>
</ul>

<p>This cycle is more than just a historical curiosity. It reflects a healthy research ecosystem where algorithms and theory co-evolve, each pushing the other forward.</p>

<h2>Classic example: the finite element method</h2>

<p>In the 1960s and 1970s, engineers began using the finite element method (FEM) for solving PDEs in structural mechanics. Early algorithms were developed based on physical intuition and computational feasibility, often with minimal theoretical backing.</p>

<p>As usage spread, mathematicians stepped in to analyze convergence, consistency, and error bounds. This theoretical work revealed the importance of mesh quality, element choice, and function spaces&mdash;leading to remedies such as adaptive mesh refinement and stabilized formulations. These in turn became subjects of rigorous mathematical analysis, culminating in the powerful FEM theory we now rely on.</p>

<h3>Modern example: neural networks</h3>

<p>The past decade has seen a similar cycle in machine learning. Deep neural networks first exploded in popularity due to empirical success in computer vision and natural language processing. The original architectures&mdash;like AlexNet&mdash;worked well, but few could explain why.</p>

<p>Soon after, theoretical work began to address generalization, expressivity, and training dynamics. Researchers identified failure modes: vanishing gradients, overfitting, sensitivity to initialization. Each insight led to remedies like residual connections, batch normalization, and dropout.</p>

<p>These new components became standard architecture elements, and new theoretical frameworks arose to understand them&mdash;such as neural tangent kernels, overparameterization theory, and mean-field limits.</p>

<h2>Why it matters</h2>

<p>Understanding this pattern helps us better appreciate the non-linear path of scientific progress. It reminds us that:</p>

<ul>
  <li><b>Empirical success can precede theory.</b><br>Not every good idea starts with a theorem.</li>
  <li><b>Failures are fertile ground.</b><br>When algorithms fail, it often sparks the most productive theoretical advances.</li>
  <li><b>Theory isn’t just for validation.</b><br>It can actively shape new methods, sometimes in surprising ways.</li>
</ul>

<p>If you’re a theorist, don’t wait for the perfect question. Look at what practitioners are doing&mdash;it’s full of open problems.</p>

<p>If you’re a practitioner, don’t be afraid to work with unproven methods. The theory might catch up&mdash;and when it does, it’ll make your work even better.</p>