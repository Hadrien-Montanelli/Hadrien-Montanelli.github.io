---
layout: post
title: Quantum computing 4
date: 2020-12-06
tags: [quantum computing]
permalink: 2020-12-06.html
---
<p>In this post, I talk about quantum computing for <a href='https://en.wikipedia.org/wiki/Supervised_learning'>supervised learning</a>. Marcello Benedetti and his colleagues wrote a very nice <a href='https://iopscience.iop.org/article/10.1088/2058-9565/ab4eb5/pdf'>review</a> last year, which I highly recommend. The main ideas are similar to those described in my previous post about quantum computing for ODEs:</p>
<ul>
<li>reformulate the problem as an <a href='https://en.wikipedia.org/wiki/Mathematical_optimization'>optimization</a> one;
<li>use hybrid systems of quantum and classical computers to solve it.
</ul></p>

<h2>Setup</h2>

<p>The core idea in supervised learning goes like this. We consider data \(\mathcal{X}\subseteq\mathbb{R}^d\), labels \(\mathcal{Y}\subseteq\mathbb{R}\) and a classifier \(f:\mathcal{X}\mapsto\mathcal{Y}\). We want to approximate \(f\) by \(\widehat{f}\) using \(n\) labeled data \((\boldsymbol{x}_1,y_1),\ldots,(\boldsymbol{x}_n,y_n)\in\mathcal{X}\times\mathcal{Y}\). The approximate classifier \(\widehat{f}\) is often parametrized by a vector of parameters \(\boldsymbol{\theta}\), and the training process consists in minimizing some <a href='https://en.wikipedia.org/wiki/Loss_function'>cost function</a> \(J(\boldsymbol{\theta})\).</p>

<p>They review two methods for quantum supervised machine learning, <i>quantum kernel estimator</i> (QKE) and <i>variational quantum model</i> (VQM). In both methods, the training set is mapped to <a href='https://en.wikipedia.org/wiki/Feature_(machine_learning)'>feature space</a> with a used-defined map \(\phi\) during the pre-processing stage.</p>

<h2>Method 1 (QKE)</h2>

<p>First, the data \(\phi(\boldsymbol{x}_i)\) is encoded in a quantum circuit by applying some operator \(U_{\phi(\boldsymbol{x}_i)}\).</p>

<p>Second, a <a href='https://en.wikipedia.org/wiki/Swap_test'>SWAP test</a> is used to evaluate the kernel.</p>

<p>Third, the evaluation of \(J(\boldsymbol{\theta})\) and the optimization over the parameters \(\boldsymbol{\theta}\) are carried out on a classical computer. A typical example of such a \(J(\boldsymbol{\theta})\) would be the following quantum <a href='https://en.wikipedia.org/wiki/Support_vector_machine'>support-vector machines</a> cost function,
$$
J(\boldsymbol{\theta}) = \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\theta_i\theta_jy_iy_j\langle K(\boldsymbol{x}_j,\boldsymbol{x}_i)\rangle-\sum_{i=1}^n\theta_i.
$$
</p>

<center>
<img src="assets/images/blog/QKE.jpg" class="img-responsive">
</center>

<h2>Method 2 (VQM)</h2>

<p>The first step is the same as in QKE.</p>

<p>Second, a variational circuit \(U(\boldsymbol{\theta})\) implements the forecasting model. This yields state \(\vert\psi_J(\boldsymbol{\theta})\rangle\) for which it is possible to measure a quantity \(\langle M_J(\boldsymbol{\theta})\rangle\) that is directly related to the forecast.</p>

<p>Third, the evaluation of both the forecast \(\widehat{f}(\boldsymbol{x}_i;\langle M_J(\boldsymbol{\theta})\rangle)\) and \(J(\boldsymbol{\theta})\), which measures the forecasting error, as well as the optimization over \(\boldsymbol{\theta}\) are carried out on a classical computer. An example of a cost function \(J(\boldsymbol{\theta})\) would be
$$
J(\boldsymbol{\theta}) = \frac{1}{n}\sum_{i=1}^n\left(\widehat{f}(\boldsymbol{x}_i;\langle M_J(\boldsymbol{\theta})\rangle)-y_i\right)^2.
$$
</p>

<center>
<img src="assets/images/blog/VQM.jpg" class="img-responsive">
</center>

<h2>Encoder circuit</h2>

<p>The encoder circuit \(U_{\phi(\boldsymbol{x})}\) can be seen as an extra layer of feature-mapping, where the inner product defines a kernel 
$$
K(\boldsymbol{x}_i,\boldsymbol{x}_j)=\vert\langle\boldsymbol{0}\vert U^{*}_{\phi(\boldsymbol{x}_j)}U_{\phi(\boldsymbol{x}_i)}\vert\boldsymbol{0}\rangle\vert^2.
$$
As I mentioned above, the SWAP test can then be used to estimate \(\langle K(\boldsymbol{x}_i,\boldsymbol{x}_j)\rangle\).</p>

<h2>Variational circuit</h2>

<p>In practice, the gate structure is fixed; only the gate parameters vary. A simple example of gate parametrization is given by
$$
U(\boldsymbol{\theta}) = \prod_{\ell=1}^L\exp\left(-\frac{i}{2}\theta_\ell P_\ell\right),
$$
where \(P_\ell\in\{I,Z,X,Y\}^{\otimes n}\) is a tensor of \(n\) <a href='https://en.wikipedia.org/wiki/Pauli_matrices'>Pauli matrices</a>.</p>

<p>It is crucial to efficiently encode the variational circuit. A fixed gate structure allows for the number of gate parameters to grow polynomially in \(n\).</p>

<p>They emphasize on two key differences between variational circuits and neural networks. The first one is that quantum circuit operations are linear, as opposed to the nonlinear <a href='https://en.wikipedia.org/wiki/Activation_function'>activation functions</a> used in <a href='https://en.wikipedia.org/wiki/Artificial_neural_network'>neural networks</a>. The second one is that it is not possible to access the quantum state during the computation, which makes it hard to design a quantum analog of <a href='https://en.wikipedia.org/wiki/Backpropagation'>backpropagation</a>.</p>

<h2>Circuit learning</h2>

<p>Circuit learning relies on standard optimization procedures, which are carried out on the classical computer. For simple gates such as the Pauli gates presented above, it is possible to derive a closed-form expression for the <a href='https://en.wikipedia.org/wiki/Gradient'>gradient</a> with respect to \(\boldsymbol{\theta}\).</p>