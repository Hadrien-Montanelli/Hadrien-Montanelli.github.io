<!DOCTYPE html>
<html lang="en">
<head>

<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=Edge">
<meta name="description" content="">
<meta name="keywords" content="">
<meta name="author" content="">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<title>Deep networks and bandlimited functions</title>
<!--

Template 2085 Neuron

http://www.tooplate.com/view/2085-neuron

-->
<link rel="stylesheet" href="css/bootstrap.min.css">
<link rel="stylesheet" href="css/font-awesome.min.css">
<link rel="stylesheet" href="css/magnific-popup.css">

<!-- Main css -->
<link rel="stylesheet" href="css/style.css">
<link href="https://fonts.googleapis.com/css?family=Lora|Merriweather:300,400" rel="stylesheet">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-108246943-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-108246943-1');
</script>

</head>
<body>

<!-- PRE LOADER -->

<div class="preloader">
     <div class="sk-spinner sk-spinner-wordpress">
          <span class="sk-inner-circle"></span>
     </div>
</div>

<!-- Navigation section  -->

<div class="navbar navbar-default navbar-static-top" role="navigation">
     <div class="container">

          <div class="navbar-header">
               <button class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                    <span class="icon icon-bar"></span>
                    <span class="icon icon-bar"></span>
                    <span class="icon icon-bar"></span>
               </button>
               <a href="index.html" class="navbar-brand">H. Montanelli</a>
          </div>
          <div class="collapse navbar-collapse">
               <ul class="nav navbar-nav navbar-right">
                    <li><a href="index.html">Home</a></li>
                    <li><a href="about.html">About</a></li>
                    <li><a href="publications.html">Publications</a></li>
                    <li><a href="talks.html">Talks</a></li>
                    <li><a href="teaching.html">Teaching</a></li>
               </ul>
          </div>

  </div>
</div>

<!-- Home Section -->

<section id="home" class="main-about parallax-section">
     <div class="overlay"></div>
     <div class="container">
          <div class="row">

               <div class="col-md-12 col-sm-12">
                    <h1>Deep networks and bandlimited functions</h1>
               </div>

          </div>
     </div>
</section>

<!-- Blog Single Post Section -->

<section id="about">
     <div class="container">
          <div class="row">

               <div class="col-md-offset-1 col-md-10 col-sm-12">     
                    <p>In a recent <a href="http://arxiv.org/pdf/1903.00735.pdf">paper</a>, my colleagues and I considered the <a href='http://en.wikipedia.org/wiki/Deep_learning'>deep</a> <a href='http://en.wikipedia.org/wiki/Rectifier_(neural_networks)'>ReLU</a> network approximation of generalized <a href='http://en.wikipedia.org/wiki/Bandlimiting'>bandlimited</a> functions \(f:B=[0,1]^d\rightarrow\mathbb{R}\) of the form
                    $$
                    f(\boldsymbol{x}) = \int_{\mathbb{R}^d}F(\boldsymbol{w})K(\boldsymbol{w}\cdot\boldsymbol{x})d\boldsymbol{w},
                    $$
                    with \(\mathrm{supp}\,F\subset[-M,M]^d\), \(M\geq1\), and for some <a href='http://en.wikipedia.org/wiki/Square-integrable_function'>square-integrable</a> function \(F:[-M,M]^d\rightarrow\mathbb{C}\) and <a href='http://en.wikipedia.org/wiki/Analytic_function'>analytic</a> kernel \(K:\mathbb{R}\rightarrow\mathbb{C}\).
                    We showed that, for any <a href='http://en.wikipedia.org/wiki/Measure_(mathematics)'>measure</a> \(\mu\), such functions can be approximated with error \(\epsilon\) in the \(L^2(B,\mu)\)-norm by deep ReLU networks of depth \(L=\mathcal{O}\left(\log_2^2\frac{1}{\epsilon}\right)\) and size \(W=\mathcal{O}\left(\frac{1}{\epsilon^2}\log_2^2\frac{1}{\epsilon}\right)\), up to some constants that depend on \(F\), \(K\), \(\mu\) and \(B\).</p>

                    <p>Our theorem is based on a result by Maurey, and on the ability of deep ReLU networks to approximate <a href='https://en.wikipedia.org/wiki/Chebyshev_polynomials'>Chebyshev polynomials</a> and <a href='http://en.wikipedia.org/wiki/Analytic_function'>analytic</a> functions efficiently.</p>

                    <h2>Maurey's theorem</h2>

                    <p>A famous <a href='http://en.wikipedia.org/wiki/Carath%C3%A9odory%27s_theorem_(convex_hull)'>theorem</a> of <a href='http://en.wikipedia.org/wiki/Constantin_Carath%C3%A9odory'>Carath&eacute;odory</a> states that if a point \(x\in\mathbb{R}^d\) lies in the <a href='http://en.wikipedia.org/wiki/Convex_hull'>convex hull</a> of a set \(P\) then \(x\) can be written as the <a href='https://en.wikipedia.org/wiki/Convex_combination'>convex combination</a> of at most \(d+1\) points in \(P\).
                    Maurey's theorem is an extension of Carath&eacute;odory's result to the infinite-dimensional case of <a href='http://en.wikipedia.org/wiki/Hilbert_space'>Hilbert spaces</a>.</p>

                    <p><b>Theorem (Maurey).</b>
                    <i>Let \(H\) be a Hilbert space with norm \(\Vert\cdot\Vert\). 
                    Suppose there exists \(G\subset H\) such that for every \(g\in G\), \(\Vert g\Vert\leq b\) for some \(b>0\). 
                    Then, for every \(f\) in the convex hull of \(G\) and every integer \(n\geq 1\), there is a \(f_n\) in the convex hull of \(n\) points in \(G\) and a constant \(c>b^2-\Vert f\Vert^2\) such that \(\Vert f - f_n\Vert^2\leq \frac{c}{n}\).</i></p>

                    <p>In practice, we used Maurey's theorem to show that there exists
                    $$\begin{align*}
                    f_{\epsilon}(\boldsymbol{x}) 
                    = \sum_{j=1}^{\lceil 1/\epsilon^2\rceil}b_j\big[&\cos(\beta_j)\mathrm{Re}(K(\boldsymbol{w}_j\cdot\boldsymbol{x})) \\
                    -\,&\sin(\beta_j)\mathrm{Im}(K(\boldsymbol{w}_j\cdot\boldsymbol{x}))\big],
                    \end{align*}
                    $$
                    with \(\sum_{j=1}^{\lceil 1/\epsilon^2\rceil}\vert b_j\vert \leq C_F\) and \(\beta_j\in\mathbb{R}\), such that
                    $$
                    \Vert f_{\epsilon}(\boldsymbol{x}) - f(\boldsymbol{x})\Vert_{L^2(\mu, B)}
                    \leq 2C_F\sqrt{\mu(B)}\epsilon.
                    $$
                    In other words, the function \(f\) is approximated by a linear combination of analytic functions with error \(2C_F\sqrt{\mu(B)}\epsilon\) in the \(L^2(B,\mu)\)-norm. The task of approximating \(f\) by deep ReLU networks has been reduced to approximating analytic functions by deep networks.</p>

                    <h2>Chebyshev polynomials, analytic functions and deep ReLU networks</h2>

                    <p>The <a href='http://en.wikipedia.org/wiki/Chebyshev_polynomials'>Chebyshev polynomials</a> of the first kind play a central role in <a href='http://en.wikipedia.org/wiki/Approximation_theory'>approximation theory</a>. They are defined on \([-1,1]\) via the three-term <a href='http://en.wikipedia.org/wiki/Recurrence_relation'>recurrence relation</a>
                    $$
                    T_n(x) = 2xT_{n-1}(x) - T_{n-2}(x), \;\; n\geq 2,
                    $$
                    with \(T_0=1\) and \(T_1(x) = x\). We showed that deep ReLU networks can implement the recurrence relation efficiently. Since truncated Chebyshev series can approximate analytic functions exponentially well, so can deep networks.</p>
               </div>

          </div>
     </div>
</section>

<!-- Footer Section -->

<footer>
     <div class="container">
          <div class="row">

               <div class="col-md-4 col-md-offset-1 col-sm-6">
                    <h3>Contact</h3>
                    <p><i class="fa fa-send-o"></i> hadrien.montanelli@gmail.com</p>
               </div>

               <div class="col-md-5 col-md-offset-1 col-sm-6">
                    <a href="https://epubs.siam.org/doi/abs/10.1137/18M1189336">
                    <img src="images/deepnet.jpg" class="img-responsive">
                    </a>
                    <div class="footer-copyright">
                         <p>Copyright &copy; 2020 Hadrien Montanelli</p>
                    </div>
               </div>

               <div class="clearfix col-md-12 col-sm-12">
                    <hr>
               </div>

               <div class="col-md-12 col-sm-12">
                    <ul class="social-icon">
                         <li><a href="http://github.com/Hadrien-Montanelli" class="fa fa-github"></a></li>
                         <li><a href="https://scholar.google.com/citations?user=Bjmkfe8AAAAJ&hl=en&oi=sra/" class="fa fa-google"></a></li>
                         <li><a href="https://www.linkedin.com/in/hadrien-montanelli/" class="fa fa-linkedin"></a></li>
                    </ul>
               </div>
               
          </div>
     </div>
</footer>

<!-- Back top -->
<a href="#back-top" class="go-top"><i class="fa fa-angle-up"></i></a>

<!-- SCRIPTS -->

<script src="js/jquery.js"></script>
<script src="js/bootstrap.min.js"></script>
<script src="js/jquery.parallax.js"></script>
<script src="js/custom.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

</body>
</html>